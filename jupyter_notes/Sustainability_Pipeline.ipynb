{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cce8f421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# i didnt have spacy or the model so I dowlaoded using this\\n!python -m spacy download en_core_web_md\\n!{sys.executable} -m pip install spacy\\n!{sys.executable} -m spacy download en\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import spacy\n",
    "import csv\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "'''\n",
    "# i didnt have spacy or the model so I dowlaoded using this\n",
    "!python -m spacy download en_core_web_md\n",
    "!{sys.executable} -m pip install spacy\n",
    "!{sys.executable} -m spacy download en\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d6c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a572dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit to Sachin for this code fr thank you\n",
    "df_UoA = pd.read_csv('./UoA-SDG-Keyword-List-Ver.-1.1.xlsx - SDG Keywords (initial ver.).csv')\n",
    "df_UoA = df_UoA[[\"SDG Keywords\", \"Alternatives\"]]\n",
    "df_UoA = pd.concat([df_UoA['SDG Keywords'], df_UoA['Alternatives']], ignore_index=True)\n",
    "df_UoA = df_UoA.to_frame(name='Keywords')\n",
    "df_UoA.dropna(inplace=True, ignore_index=True)\n",
    "\n",
    "df_USC = pd.read_csv('./USC-Keywords.csv')\n",
    "df_USC = pd.concat([df_USC[col] for col in df_USC.columns], ignore_index=True)\n",
    "df_USC = df_USC.to_frame(name='Keywords')\n",
    "df_USC.dropna(inplace=True, ignore_index=True)\n",
    "\n",
    "# combine keywords\n",
    "combined_df = pd.concat([df_UoA, df_USC], ignore_index=True)\n",
    "combined_df.dropna(inplace=True, ignore_index=True)\n",
    "keywords_list = combined_df['Keywords'].tolist()\n",
    "keywords = set(keywords_list)\n",
    "keywords = sorted(list(keywords))\n",
    "\n",
    "for i,phrase in enumerate(keywords):\n",
    "    if \";\" in phrase:\n",
    "        segments = phrase.split(\";\")\n",
    "        segments = [f\"({segment.strip()})\" for segment in segments]\n",
    "        new_str = \" OR \".join(segments)\n",
    "        keywords[i] = new_str\n",
    "    if \" / \" in phrase:\n",
    "        segments = phrase.split(\" / \")\n",
    "        segments = [f\"({segment.strip()})\" for segment in segments]\n",
    "        new_str = \" OR \".join(segments)\n",
    "        keywords[i] = new_str\n",
    "\n",
    "len(keywords)\n",
    "\n",
    "pd.DataFrame(keywords, columns=[\"Keywords\"]).to_csv('cleaned_keywords.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6328f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model so we can try to use during matching\n",
    "# 'en_core...' is just loading the english model\n",
    "# nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da506650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading elastic search\n",
    "es = Elasticsearch(\n",
    "    # this needs to come from the dashboard of the project itself\n",
    "    cloud_id='==',  # Find this in your Elastic Cloud console\n",
    "    basic_auth=('elastic', '')\n",
    ")\n",
    "# used the id for the place where i indexed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1da08823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# courses are done so now we can bulkload courses into elastic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "451633e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bulk load our courses into elastic search\n",
    "def index_courses(courses_csv):\n",
    "    #read courses in\n",
    "    courses_df = pd.read_csv(courses_csv)\n",
    "    actions = [\n",
    "        {\n",
    "            #index our courses\n",
    "            \"_index\": \"courses\",\n",
    "            \"_source\": {\n",
    "                # specify\n",
    "                \"abrev\": row[\"Abrev\"],\n",
    "                \"CourseReferenceNumber\": row[\"courseReferenceNumber\"],\n",
    "                \"Description\": row[\"description\"]\n",
    "            }\n",
    "        }\n",
    "        for _, row in courses_df.iterrows()\n",
    "    ]\n",
    "    helpers.bulk(es, actions)\n",
    "    \n",
    "#lets use this for our courses!!\n",
    "index_courses(\"courses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d343db69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to try to use re module and regex to clean up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e8fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43cd3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess text\n",
    "def preprocess_cleaned(text):\n",
    "    text = text.lower()  \n",
    "    # rmove punctuation and special characters including * and ?\n",
    "    # text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "    text = re.sub(r'[?*]', '', text)  \n",
    "    # what if we add if/else so that where space = \"AND\", / = \"OR\"? ?????????????????????????? pseduo code\n",
    "    '''\n",
    "    procesed = []\n",
    "    keywords_df = pd.read_csv(keywords_csv)\n",
    "    keywords = keywords_df['Keywords'].tolist()\n",
    "    for keyword in keywords\n",
    "        if '\\'\n",
    "            we can use regex sub and also .split() together\n",
    "            https://stackoverflow.com/questions/56486564/how-to-replace-specific-words-from-entire-csv-file\n",
    "            sort of like this file^\n",
    "    '''\n",
    "    return text\n",
    "\n",
    "# function to load and preprocess keywords\n",
    "def load_and_preprocess_keywords(keywords_csv):\n",
    "    # Read keywords from csv\n",
    "    keywords_df = pd.read_csv(keywords_csv)\n",
    "    keywords = keywords_df['Keywords'].tolist()\n",
    "\n",
    "    # apply preprocessing\n",
    "    preprocess_cleaned_list = [preprocess_cleaned(keyword) for keyword in keywords]\n",
    "    \n",
    "    # save preprocessed keywords back to csv\n",
    "    pd.DataFrame(preprocess_cleaned_list, columns=[\"Keywords\"]).to_csv(keywords_csv, index=False)\n",
    "    \n",
    "    return preprocess_cleaned_list\n",
    "\n",
    "# finally, let's use the function\n",
    "preprocessed_keywords = load_and_preprocess_keywords('cleaned_keywords.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7e89f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>food supply chain management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>food supply chains or fsc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>food system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>food systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>food value chain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3460</th>\n",
       "      <td>local materials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3461</th>\n",
       "      <td>mitigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>per capita gdp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>per capita gross domestic product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>wellbeingwell beingwellbeing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2265 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Keywords\n",
       "1200       food supply chain management\n",
       "1201          food supply chains or fsc\n",
       "1202                        food system\n",
       "1203                       food systems\n",
       "1204                   food value chain\n",
       "...                                 ...\n",
       "3460                    local materials\n",
       "3461                         mitigation\n",
       "3462                     per capita gdp\n",
       "3463  per capita gross domestic product\n",
       "3464       wellbeingwell beingwellbeing\n",
       "\n",
       "[2265 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' df looks good, no * or ?, so im going to jump into vector search \n",
    "wiht the cleaned keywords and then the courses'''\n",
    "df_clean = pd.read_csv('cleaned_keywords.csv')\n",
    "df_clean[1200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3da754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' in order to use vector search we need to embed the keywords\n",
    "ad make sure they can be placed into/ tranfsored into numerical vectors\n",
    "I will be using hugging face model to map sentences to dimensional space\n",
    "https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2'''\n",
    "\n",
    "# loading model, took from huggin face. Powered by Bert :O\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# vectorizing keywords\n",
    "def embed_words(preprocessed_keywords):\n",
    "    # set up from hugging face\n",
    "    # encoding keywords\n",
    "    embeddings = model.encode(preprocessed_keywords, convert_to_tensor = True)\n",
    "    return embeddings\n",
    "\n",
    "# keywords in vector\n",
    "embedded_keywords = embed_words(preprocessed_keywords)\n",
    "\n",
    "#look into normallization for embeddings **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e49a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have to embed course descriptions\n",
    "def embed_courses (courses, indexes):\n",
    "    # has to be different bc courses has more than just desciptions\n",
    "    pd.read_csv(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03bef95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = embedded_keywords[1202]\n",
    "y = embedded_keywords[1204]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2bceac4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6142293"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "norm_x = np.linalg.norm(x)\n",
    "norm_y = np.linalg.norm(y)\n",
    "\n",
    "np.dot(x/norm_x, y/norm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d02a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a plot of all vectors using tsneat?\n",
    "# llm using spacy and incorperate LLM to categorize as SDG and then \n",
    "\n",
    "# OR we can make a vector of SGD's and add those to the appropriate words/vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
