{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Elasticsearch and Generating Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "from elasticsearch.exceptions import RequestError\n",
    "\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conect to Elasticsearch API\n",
    "- Create and import environment variables `ELASTIC_SEARCH_API_ENDPOINT` and `ELASTIC_SEARCH_API_KEY`\n",
    "- Creates Index if it doesn't already exist\n",
    "- Note: Settings are set to `my_ascii_folding_filter` which means elastic search will treat accented characters as it's non-accented verision and itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Info: {'name': 'instance-0000000000', 'cluster_name': 'a6d98f7b24ab431881958cb334894f4f', 'cluster_uuid': 'ToWvQ6beRcCo4O74i9TPtA', 'version': {'number': '8.14.2', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '2afe7caceec8a26ff53817e5ed88235e90592a1b', 'build_date': '2024-07-01T22:06:58.515911606Z', 'build_snapshot': False, 'lucene_version': '9.10.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n",
      "Index 'courses' already exists.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index = \"courses\"\n",
    "load_dotenv() \n",
    "client = Elasticsearch(\n",
    "  os.getenv(\"ELASTIC_SEARCH_API_ENDPOINT\"),\n",
    "  api_key=os.getenv(\"ELASTIC_SEARCH_API_KEY\"),\n",
    "  # verify_certs=False\n",
    ")\n",
    "try:\n",
    "  print(\"Client Info:\", client.info())\n",
    "except ElasticsearchException as e:\n",
    "  print(f\"Error connecting to Elasticsearch: {e}\")\n",
    "\n",
    "settings = {\n",
    "    \"analysis\": {\n",
    "        \"filter\": {\n",
    "            \"my_ascii_folding_filter\": {\n",
    "                \"type\": \"asciifolding\",\n",
    "                \"preserve_original\": True\n",
    "            }\n",
    "        },\n",
    "        \"analyzer\": {\n",
    "            \"standard_asciifolding\": {\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\"lowercase\", \"my_ascii_folding_filter\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "mappings = {\n",
    "    \"properties\": {\n",
    "        \"course\": {\"type\": \"text\", \"analyzer\": \"standard_asciifolding\"},\n",
    "        \"CRN\": {\"type\": \"text\", \"analyzer\": \"standard_asciifolding\"},\n",
    "        \"subjectDescription\": {\"type\": \"text\", \"analyzer\": \"standard_asciifolding\"},\n",
    "        \"courseTitle\": {\"type\": \"text\", \"analyzer\": \"standard_asciifolding\"},\n",
    "        \"description\": {\"type\": \"text\", \"analyzer\": \"standard_asciifolding\"},\n",
    "    }\n",
    "}\n",
    "\n",
    "try:\n",
    "    client.indices.create(index=index, mappings=mappings, settings=settings)\n",
    "    print(f\"Index '{index}' created successfully.\")\n",
    "except RequestError:\n",
    "    print(f\"Index '{index}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bulk Load Data into Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1685, [])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_courses = pd.read_csv(\"courses.csv\")\n",
    "def generate_docs():\n",
    "    for _, row in df_courses.iterrows():\n",
    "        yield {\n",
    "            \"_index\": index,\n",
    "            \"_id\":row[\"courseNumber\"],\n",
    "            \"CRN\": row[\"courseReferenceNumber\"],\n",
    "            \"courseTitle\": row[\"courseTitle\"],\n",
    "            \"subjectDescription\": row[\"subjectDescription\"],\n",
    "            \"description\": row[\"description\"],\n",
    "        }\n",
    "\n",
    "helpers.bulk(client, generate_docs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Keywords/Phrases\n",
    "Reasoning behind data cleaning decisions\n",
    "- Query search default operator is `AND` because `OR` leads to many redundancies \n",
    "- `{}` were replaced with `()` because elasticsearch query search doesn't allow `{}`\n",
    "\n",
    "* `;` were replaced with ` OR ` and `()` wraping the phrases between each `;` in because there are multiple phrases in 1 entry\n",
    "  - For Example: `\"Poverty Alleviation Programs; Poverty Alleviation Programme; Poverty Alleviation Programmes\"` is cleaned to `\"(Poverty Alleviation Programs) OR (Poverty Alleviation Programme) OR (Poverty Alleviation Programmes)\"`\n",
    "- `\" / \"` is converted to `OR` and `()` wraping the phrases between each `\" / \"` because there are multiple phrases in 1 entry\n",
    "  - For Example: `\"Doha Development Round / Doha Round\"` is converted to `\"(Doha Development Round) OR (Doha Round)\"`\n",
    "- `\"/\"` is converted to `OR` and no `()` wraping the phrases between each `\"/\"` because `\"/\"` separtes two words instead of two phrases like with `\" / \"`\n",
    "  - For Example: `\"Gender equality/parity\"` is converted to `\"(Gender equality OR parity)\"`\n",
    "  - Note: 1 inconsistent entry in USC keywords list is `\"Sustainable building/s\"` and this is converted to `\"Sustainable building OR s`\n",
    "  - Extra Possible Solution: Replace `/` with wildcard `*` in order to allow for more possible words, however search time may increase\n",
    "- `\"-\"` is converted to `OR` and no `()` wraping the words\n",
    "  - For Example: `\"Micro-organisms\"` is converted to `\"Micro OR organisms\"`\n",
    "  - Note: Change to `AND` if for both parts of hyphenated word need to be in course \n",
    "  - Extra Possible Solution: Replace `-` with wildcard `*` in order to allow for more possible words, however search time may increase\n",
    "* All strings are unicode normalized to `NFKC` standard in order to replace non-breaking spaces with regular spaces\n",
    "  - For Example: `\\xa0` is a non-breaking space and it is converted to `\" \"` \n",
    "* All stop words are removed using the `NLTK` library because common words such as lowercase `\"and, \"or\", \"not\", be, being, is, the, etc...` should not be requred to be in search query\n",
    "  - Note: 1 entry in USC keywords list is `\"wellbeing/well being/well-being\"` and the stop words library did remove `\"being`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/safeduck/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Words: {'Over', 'Not', 'the', 'in', 'No', 'of', 'In', 'against', 'To', 'being', 'All', 'and', 'The', 's', 'or', 'no', 'to', 'And', 'with', 'Being', 'all', 'for', 'Under', 'For', 'on', 'Against'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/safeduck/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3465"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_UoA = pd.read_csv('./UoA-SDG-Keyword-List-Ver.-1.1.xlsx - SDG Keywords (initial ver.).csv')\n",
    "df_UoA = df_UoA[[\"SDG Keywords\", \"Alternatives\"]]\n",
    "df_UoA = pd.concat([df_UoA['SDG Keywords'], df_UoA['Alternatives']], ignore_index=True)\n",
    "df_UoA = df_UoA.to_frame(name='Keywords')\n",
    "df_UoA.dropna(inplace=True, ignore_index=True)\n",
    "\n",
    "df_USC = pd.read_csv('./USC-Compiled-Keywords-for-SDG-Mapping_Final_17-05-10.xlsx - Compiled SDG Keywords.csv')\n",
    "df_USC = pd.concat([df_USC[col] for col in df_USC.columns], ignore_index=True)\n",
    "df_USC = df_USC.to_frame(name='Keywords')\n",
    "df_USC.dropna(inplace=True, ignore_index=True)\n",
    "\n",
    "combined_df = pd.concat([df_UoA, df_USC], ignore_index=True)\n",
    "combined_df.dropna(inplace=True, ignore_index=True)\n",
    "keywords_list = combined_df['Keywords'].tolist()\n",
    "keywords = set(keywords_list)\n",
    "keywords = sorted(list(keywords))\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "removed_stop_words = set()\n",
    "\n",
    "for i, phrase in enumerate(keywords):\n",
    "    new_str = phrase\n",
    "\n",
    "    if \"{\" in phrase:\n",
    "        new_str = new_str.replace(\"{\", \"(\")\n",
    "    if \"}\" in phrase:\n",
    "        new_str = new_str.replace(\"}\", \")\")\n",
    "\n",
    "    if \";\" in phrase:\n",
    "        segments = new_str.split(\";\")\n",
    "        segments = [f\"({segment.strip()})\" for segment in segments]\n",
    "        new_str = \" OR \".join(segments)\n",
    "        \n",
    "    if \" / \" in phrase:\n",
    "        segments = new_str.split(\" / \")\n",
    "        segments = [f\"({segment.strip()})\" for segment in segments]\n",
    "        new_str = \" OR \".join(segments)\n",
    "        \n",
    "    if \"/\" in phrase:\n",
    "        segments = new_str.split(\"/\")\n",
    "        segments = [f\"{segment.strip()}\" for segment in segments]\n",
    "        new_str = \" OR \".join(segments)\n",
    "    \n",
    "    if \"-\" in phrase:\n",
    "        segments = new_str.split(\"-\")\n",
    "        segments = [f\"{segment.strip()}\" for segment in segments]\n",
    "        new_str = \" OR \".join(segments) \n",
    "\n",
    "    #normalize the data\n",
    "    new_str = unicodedata.normalize('NFKC', new_str)\n",
    "\n",
    "    #remove stop words except for AND, OR, and NOT\n",
    "    words = new_str.split(\" \")\n",
    "    filtered_words = [\n",
    "        word for word in words \n",
    "        if word.lower() not in stop_words or word.isupper()\n",
    "    ]\n",
    "    removed_stop_words.update([\n",
    "        word for word in words \n",
    "        if word.lower() in stop_words and not word.isupper()\n",
    "    ])\n",
    "    new_str = ' '.join(filtered_words)\n",
    "    keywords[i] = new_str\n",
    "print(\"Removed Words:\", removed_stop_words)\n",
    "\n",
    "\n",
    "\n",
    "len(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Keywords to CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('keywords.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for item in keywords:\n",
    "        writer.writerow([item])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Report\n",
    "- The file `report.txt` contains the all keywords with matching courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_matches = 0\n",
    "courses_matched = set()\n",
    "with open('report.txt', 'w') as f:\n",
    "    for phrase in keywords:\n",
    "        result = client.search(index=\"courses\", query={\"query_string\": {\"query\": phrase.lower(), \"default_field\": \"description\",\"analyze_wildcard\": True, \"default_operator\": \"AND\"}}, size=10000)\n",
    "        total_matches += result[\"hits\"][\"total\"][\"value\"]\n",
    "        \n",
    "        f.write(f\"-------- {phrase} --------\\n\")\n",
    "        f.write(f\"Total Class Count: {len(result['hits']['hits'])}\\n\")\n",
    "\n",
    "        for hit in result[\"hits\"][\"hits\"]:\n",
    "            courseNumber = hit[\"_id\"]\n",
    "            courses_matched.add(courseNumber)\n",
    "            courseTitle = hit[\"_source\"][\"courseTitle\"]\n",
    "            subjectDescription = hit[\"_source\"][\"subjectDescription\"]\n",
    "            f.write(f\"{subjectDescription}: {courseNumber} - {courseTitle}\\n\")\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    f.write(f\"Total matches: {total_matches}\\n\")\n",
    "    f.write(f\"Total courses: {len(courses_matched)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
